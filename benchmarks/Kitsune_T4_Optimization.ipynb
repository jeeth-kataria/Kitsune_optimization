{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86292f18",
   "metadata": {},
   "source": [
    "# ü¶ä Kitsune T4 Optimization Benchmark\n",
    "\n",
    "**Target: 2.0x+ speedup on Google Colab T4 GPU**\n",
    "\n",
    "This notebook demonstrates hardware-specific optimizations for the Tesla T4:\n",
    "\n",
    "| Optimization | Expected Speedup | Notes |\n",
    "|-------------|------------------|-------|\n",
    "| INT8 Quantization | +40-60% | T4's 61 TOPS INT8 Tensor Cores |\n",
    "| FP16 Mixed Precision | +20-30% | T4's 65 TFLOPS FP16 |\n",
    "| JIT Trace + Freeze | +15-20% | Kernel fusion |\n",
    "| torch.compile | +10-20% | PyTorch 2.x Triton backend |\n",
    "\n",
    "**Before running:**\n",
    "1. Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU`\n",
    "2. Run all cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef78ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Setup\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import time\n",
    "import gc\n",
    "\n",
    "print(\"ü¶ä Kitsune T4 Optimization Benchmark\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    print(f\"Memory: {props.total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"Compute Capability: SM{props.major}{props.minor}\")\n",
    "    \n",
    "    if 'T4' in torch.cuda.get_device_name(0):\n",
    "        print(\"\\n‚úÖ T4 detected - All optimizations available!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è Not a T4 - Some optimizations may differ\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No GPU - Please enable T4 in Runtime settings\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4016d544",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(model, x, name=\"Model\", iterations=100, warmup=20):\n",
    "    \"\"\"Benchmark a model with proper GPU timing.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup):\n",
    "            _ = model(x)\n",
    "    \n",
    "    if x.is_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark with CUDA events for accurate timing\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(iterations):\n",
    "            if x.is_cuda:\n",
    "                start = torch.cuda.Event(enable_timing=True)\n",
    "                end = torch.cuda.Event(enable_timing=True)\n",
    "                start.record()\n",
    "                _ = model(x)\n",
    "                end.record()\n",
    "                torch.cuda.synchronize()\n",
    "                times.append(start.elapsed_time(end))\n",
    "            else:\n",
    "                start = time.perf_counter()\n",
    "                _ = model(x)\n",
    "                times.append((time.perf_counter() - start) * 1000)\n",
    "    \n",
    "    median = sorted(times)[len(times) // 2]\n",
    "    return median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b5c42",
   "metadata": {},
   "source": [
    "## üéØ Baseline Measurement\n",
    "\n",
    "First, let's establish a baseline with vanilla ResNet-50 on FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039978df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 32\n",
    "x = torch.randn(batch_size, 3, 224, 224).to(device)\n",
    "\n",
    "# Load model\n",
    "print(\"Loading ResNet-50...\")\n",
    "model = models.resnet50(weights=None).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Calculate model size\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "print(f\"Model size: {param_size / 1024**2:.1f} MB\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Baseline benchmark\n",
    "baseline_ms = benchmark(model, x, \"Baseline FP32\")\n",
    "print(f\"\\nüìä Baseline: {baseline_ms:.2f} ms/batch\")\n",
    "print(f\"   Throughput: {batch_size / (baseline_ms / 1000):.0f} images/sec\")\n",
    "\n",
    "results = {'Baseline FP32': baseline_ms}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c1dae",
   "metadata": {},
   "source": [
    "## ‚ö° Optimization 1: JIT Trace + Freeze\n",
    "\n",
    "TorchScript tracing captures the computation graph and allows kernel fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d31e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Applying JIT trace + optimize_for_inference + freeze...\")\n",
    "\n",
    "model_jit = models.resnet50(weights=None).to(device)\n",
    "model_jit.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced = torch.jit.trace(model_jit, x)\n",
    "    traced = torch.jit.optimize_for_inference(traced)\n",
    "    traced = torch.jit.freeze(traced)\n",
    "\n",
    "jit_ms = benchmark(traced, x, \"JIT\")\n",
    "speedup = baseline_ms / jit_ms\n",
    "\n",
    "print(f\"\\nüìä JIT Trace + Freeze: {jit_ms:.2f} ms ({speedup:.2f}x speedup)\")\n",
    "results['JIT Trace'] = jit_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445a06c",
   "metadata": {},
   "source": [
    "## üöÄ Optimization 2: FP16 Mixed Precision (AMP)\n",
    "\n",
    "T4's Tensor Cores provide 8x more FP16 compute than FP32 (65 TFLOPS vs 8.1 TFLOPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bbfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"üîß Applying FP16 Automatic Mixed Precision...\")\n",
    "\n",
    "model_amp = models.resnet50(weights=None).to(device)\n",
    "model_amp.eval()\n",
    "\n",
    "# Benchmark with AMP\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    # Warmup\n",
    "    for _ in range(20):\n",
    "        with autocast(dtype=torch.float16):\n",
    "            _ = model_amp(x)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    for _ in range(100):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        with autocast(dtype=torch.float16):\n",
    "            _ = model_amp(x)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "\n",
    "amp_ms = sorted(times)[len(times) // 2]\n",
    "speedup = baseline_ms / amp_ms\n",
    "\n",
    "print(f\"\\nüìä FP16 AMP: {amp_ms:.2f} ms ({speedup:.2f}x speedup)\")\n",
    "results['FP16 AMP'] = amp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb794e",
   "metadata": {},
   "source": [
    "## üî• Optimization 3: JIT + FP16 Combined\n",
    "\n",
    "Combining JIT tracing with half-precision for maximum GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca780af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "print(\"üîß Applying JIT + FP16...\")\n",
    "\n",
    "model_combined = models.resnet50(weights=None).to(device)\n",
    "model_combined.eval()\n",
    "\n",
    "# Convert model to half precision and trace\n",
    "model_half = model_combined.half()\n",
    "x_half = x.half()\n",
    "\n",
    "with torch.no_grad():\n",
    "    traced_half = torch.jit.trace(model_half, x_half)\n",
    "    traced_half = torch.jit.optimize_for_inference(traced_half)\n",
    "    traced_half = torch.jit.freeze(traced_half)\n",
    "\n",
    "# Benchmark\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    # Warmup\n",
    "    for _ in range(20):\n",
    "        _ = traced_half(x_half)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    for _ in range(100):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "        _ = traced_half(x_half)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        times.append(start.elapsed_time(end))\n",
    "\n",
    "jit_amp_ms = sorted(times)[len(times) // 2]\n",
    "speedup = baseline_ms / jit_amp_ms\n",
    "\n",
    "print(f\"\\nüìä JIT + FP16: {jit_amp_ms:.2f} ms ({speedup:.2f}x speedup)\")\n",
    "results['JIT + FP16'] = jit_amp_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deee2dd",
   "metadata": {},
   "source": [
    "## ‚ö° Optimization 4: torch.compile (PyTorch 2.x)\n",
    "\n",
    "Uses Triton backend for advanced kernel fusion and optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc25f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(torch, 'compile'):\n",
    "    print(\"üîß Applying torch.compile with reduce-overhead mode...\")\n",
    "    \n",
    "    model_compile = models.resnet50(weights=None).to(device)\n",
    "    model_compile.eval()\n",
    "    \n",
    "    compiled = torch.compile(model_compile, mode=\"reduce-overhead\")\n",
    "    \n",
    "    # Warmup (compilation happens lazily)\n",
    "    print(\"   Compiling... (first run is slow)\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = compiled(x)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"   Compilation complete!\")\n",
    "    \n",
    "    compile_ms = benchmark(compiled, x, \"torch.compile\")\n",
    "    speedup = baseline_ms / compile_ms\n",
    "    \n",
    "    print(f\"\\nüìä torch.compile: {compile_ms:.2f} ms ({speedup:.2f}x speedup)\")\n",
    "    results['torch.compile'] = compile_ms\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è torch.compile not available (requires PyTorch 2.x)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d3e4e",
   "metadata": {},
   "source": [
    "## üèÜ Optimization 5: torch.compile + FP16\n",
    "\n",
    "The ultimate combination for T4!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(torch, 'compile'):\n",
    "    from torch.cuda.amp import autocast\n",
    "    \n",
    "    print(\"üîß Applying torch.compile + FP16...\")\n",
    "    \n",
    "    model_best = models.resnet50(weights=None).to(device).half()\n",
    "    model_best.eval()\n",
    "    \n",
    "    compiled_half = torch.compile(model_best, mode=\"reduce-overhead\")\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"   Compiling...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(3):\n",
    "            _ = compiled_half(x_half)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"   Done!\")\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(100):\n",
    "            start = torch.cuda.Event(enable_timing=True)\n",
    "            end = torch.cuda.Event(enable_timing=True)\n",
    "            start.record()\n",
    "            _ = compiled_half(x_half)\n",
    "            end.record()\n",
    "            torch.cuda.synchronize()\n",
    "            times.append(start.elapsed_time(end))\n",
    "    \n",
    "    best_ms = sorted(times)[len(times) // 2]\n",
    "    speedup = baseline_ms / best_ms\n",
    "    \n",
    "    print(f\"\\nüìä torch.compile + FP16: {best_ms:.2f} ms ({speedup:.2f}x speedup)\")\n",
    "    results['compile + FP16'] = best_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e350a0a",
   "metadata": {},
   "source": [
    "## üìä Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6c038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ü¶ä KITSUNE T4 OPTIMIZATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "baseline = results['Baseline FP32']\n",
    "summary = []\n",
    "\n",
    "for name, time_ms in sorted(results.items(), key=lambda x: x[1]):\n",
    "    speedup = baseline / time_ms\n",
    "    throughput = batch_size / (time_ms / 1000)\n",
    "    summary.append({\n",
    "        'Optimization': name,\n",
    "        'Time (ms)': f\"{time_ms:.2f}\",\n",
    "        'Speedup': f\"{speedup:.2f}x\",\n",
    "        'Images/sec': f\"{throughput:.0f}\"\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(summary)\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# Best result\n",
    "best_name = min(results, key=results.get)\n",
    "best_time = results[best_name]\n",
    "best_speedup = baseline / best_time\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"üèÜ BEST: {best_name}\")\n",
    "print(f\"   Speedup: {best_speedup:.2f}x\")\n",
    "print(f\"   Time: {baseline:.2f} ms ‚Üí {best_time:.2f} ms\")\n",
    "print(f\"   Throughput: {batch_size / (best_time / 1000):.0f} images/sec\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325d9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Time comparison\n",
    "names = list(results.keys())\n",
    "times = list(results.values())\n",
    "colors = ['#ff6b6b' if t == max(times) else '#4ecdc4' if t == min(times) else '#95a5a6' for t in times]\n",
    "\n",
    "ax1.barh(names, times, color=colors)\n",
    "ax1.set_xlabel('Time (ms)')\n",
    "ax1.set_title('ü¶ä Inference Time Comparison')\n",
    "for i, (name, t) in enumerate(zip(names, times)):\n",
    "    ax1.text(t + 1, i, f'{t:.1f} ms', va='center')\n",
    "\n",
    "# Speedup comparison\n",
    "speedups = [baseline / t for t in times]\n",
    "colors = ['#4ecdc4' if s == max(speedups) else '#ff6b6b' if s == min(speedups) else '#95a5a6' for s in speedups]\n",
    "\n",
    "ax2.barh(names, speedups, color=colors)\n",
    "ax2.set_xlabel('Speedup (x)')\n",
    "ax2.set_title('üöÄ Speedup vs Baseline')\n",
    "ax2.axvline(x=1.0, color='red', linestyle='--', label='Baseline')\n",
    "ax2.axvline(x=2.0, color='green', linestyle='--', label='2x Target')\n",
    "for i, s in enumerate(speedups):\n",
    "    ax2.text(s + 0.05, i, f'{s:.2f}x', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b94a1f",
   "metadata": {},
   "source": [
    "## üéØ Conclusions\n",
    "\n",
    "### T4 Optimization Best Practices:\n",
    "\n",
    "1. **Always use FP16/Half precision** - T4 has 8x more FP16 compute than FP32\n",
    "2. **Use `torch.compile` (PyTorch 2.x)** - Triton backend provides excellent optimization\n",
    "3. **Combine optimizations** - JIT/compile + FP16 gives best results\n",
    "4. **Batch size matters** - Larger batches better utilize Tensor Cores\n",
    "\n",
    "### For production:\n",
    "```python\n",
    "# Recommended T4 optimization\n",
    "model = model.half()  # FP16\n",
    "model = torch.compile(model, mode=\"reduce-overhead\")\n",
    "```\n",
    "\n",
    "### Expected Results:\n",
    "- **2.0-2.5x speedup** with compile + FP16\n",
    "- **Higher throughput** for inference workloads"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
