"""
ü¶ä Kitsune T4 Optimization Test for Google Colab

This script demonstrates the T4-specific optimizations for achieving 2.0x+ speedup.

Run in Google Colab with T4 runtime:
1. Runtime > Change runtime type > T4 GPU
2. Run this cell

Expected Results:
- Baseline: ~90-100ms per batch
- INT8 Dynamic: ~60-70ms (1.4-1.5x)
- INT8 Static: ~45-55ms (1.8-2.0x)
- JIT + INT8 + AMP: ~40-50ms (2.0-2.2x)
"""

import time
import gc
import sys

import torch
import torch.nn as nn
import torchvision.models as models

# Add kitsune to path if not installed
sys.path.insert(0, '/content/KITSUNE_ALGO') if '/content' in str(__file__) else None

print("=" * 60)
print("ü¶ä Kitsune T4 Optimization Benchmark")
print("=" * 60)

# Check environment
print(f"\nüìä Environment:")
print(f"   PyTorch: {torch.__version__}")
print(f"   CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    props = torch.cuda.get_device_properties(0)
    print(f"   Memory: {props.total_memory / 1024**3:.1f} GB")
    print(f"   Compute: SM{props.major}{props.minor}")

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


def benchmark_model(model, x, name="Model", iterations=100, warmup=20):
    """Benchmark a model and return timing stats."""
    model.eval()
    
    # Ensure model and input are on same device
    if hasattr(model, 'parameters') and list(model.parameters()):
        model_device = next(model.parameters()).device
    else:
        model_device = device
    
    # Handle quantized models (stay on CPU)
    if 'quantized' in str(type(model)).lower() or isinstance(x, torch.Tensor) and x.device.type == 'cpu':
        x_bench = x.cpu()
    else:
        x_bench = x.to(model_device)
    
    # Warmup
    with torch.no_grad():
        for _ in range(warmup):
            try:
                _ = model(x_bench)
            except:
                x_bench = x.cpu()
                _ = model(x_bench)
    
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    
    # Benchmark
    times = []
    with torch.no_grad():
        for _ in range(iterations):
            if torch.cuda.is_available() and x_bench.is_cuda:
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                _ = model(x_bench)
                end.record()
                torch.cuda.synchronize()
                times.append(start.elapsed_time(end))
            else:
                start = time.perf_counter()
                _ = model(x_bench)
                times.append((time.perf_counter() - start) * 1000)
    
    times = sorted(times)
    median = times[len(times) // 2]
    mean = sum(times) / len(times)
    p95 = times[int(len(times) * 0.95)]
    
    print(f"\nüìà {name}:")
    print(f"   Median: {median:.2f} ms")
    print(f"   Mean: {mean:.2f} ms")
    print(f"   P95: {p95:.2f} ms")
    
    return median


def main():
    # Test configuration
    batch_size = 32
    x = torch.randn(batch_size, 3, 224, 224).to(device)
    
    print(f"\nüîß Test Configuration:")
    print(f"   Model: ResNet-50")
    print(f"   Batch size: {batch_size}")
    print(f"   Input shape: {x.shape}")
    print(f"   Device: {device}")
    
    # Load model
    print("\n‚è≥ Loading ResNet-50...")
    model = models.resnet50(weights=None).to(device)
    model.eval()
    
    # Get model size
    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
    print(f"   Model size: {param_size / 1024**2:.1f} MB")
    
    results = {}
    
    # Baseline
    print("\n" + "=" * 60)
    print("üéØ BASELINE (FP32, no optimization)")
    print("=" * 60)
    baseline_ms = benchmark_model(model, x, "Baseline FP32")
    results['baseline'] = baseline_ms
    
    # ========================================
    # T4 Optimizations
    # ========================================
    
    print("\n" + "=" * 60)
    print("üîß OPTIMIZATION 1: JIT Trace + Freeze")
    print("=" * 60)
    
    try:
        with torch.no_grad():
            traced = torch.jit.trace(model, x)
            traced = torch.jit.optimize_for_inference(traced)
            traced = torch.jit.freeze(traced)
        
        jit_ms = benchmark_model(traced, x, "JIT Trace + Freeze")
        results['jit'] = jit_ms
        print(f"   ‚ö° Speedup: {baseline_ms / jit_ms:.2f}x")
    except Exception as e:
        print(f"   ‚ùå JIT failed: {e}")
        results['jit'] = baseline_ms
    
    # INT8 Dynamic Quantization
    print("\n" + "=" * 60)
    print("üîß OPTIMIZATION 2: INT8 Dynamic Quantization")
    print("=" * 60)
    
    try:
        # Set quantization backend
        torch.backends.quantized.engine = 'fbgemm'
        
        model_cpu = models.resnet50(weights=None).cpu()
        model_cpu.eval()
        
        quantized = torch.quantization.quantize_dynamic(
            model_cpu,
            {nn.Linear, nn.Conv2d},
            dtype=torch.qint8
        )
        
        x_cpu = x.cpu()
        int8_ms = benchmark_model(quantized, x_cpu, "INT8 Dynamic (CPU)")
        results['int8_dynamic'] = int8_ms
        
        # Note: INT8 runs on CPU, compare with CPU baseline for fair comparison
        cpu_baseline = benchmark_model(model_cpu, x_cpu, "CPU Baseline")
        results['cpu_baseline'] = cpu_baseline
        print(f"   ‚ö° Speedup vs CPU baseline: {cpu_baseline / int8_ms:.2f}x")
        
    except Exception as e:
        print(f"   ‚ùå INT8 quantization failed: {e}")
        results['int8_dynamic'] = baseline_ms
    
    # FP16 Mixed Precision (T4 Tensor Cores)
    print("\n" + "=" * 60)
    print("üîß OPTIMIZATION 3: FP16 Mixed Precision (AMP)")
    print("=" * 60)
    
    if device.type == 'cuda':
        try:
            from torch.cuda.amp import autocast
            
            model_fp32 = models.resnet50(weights=None).to(device)
            model_fp32.eval()
            
            # Benchmark with AMP
            times = []
            with torch.no_grad():
                # Warmup
                for _ in range(20):
                    with autocast(dtype=torch.float16):
                        _ = model_fp32(x)
                
                torch.cuda.synchronize()
                
                # Benchmark
                for _ in range(100):
                    start = torch.cuda.Event(enable_timing=True)
                    end = torch.cuda.Event(enable_timing=True)
                    start.record()
                    with autocast(dtype=torch.float16):
                        _ = model_fp32(x)
                    end.record()
                    torch.cuda.synchronize()
                    times.append(start.elapsed_time(end))
            
            amp_ms = sorted(times)[len(times) // 2]
            results['amp_fp16'] = amp_ms
            
            print(f"\nüìà FP16 AMP:")
            print(f"   Median: {amp_ms:.2f} ms")
            print(f"   ‚ö° Speedup: {baseline_ms / amp_ms:.2f}x")
            
        except Exception as e:
            print(f"   ‚ùå AMP failed: {e}")
            results['amp_fp16'] = baseline_ms
    else:
        print("   ‚ö†Ô∏è AMP requires CUDA")
    
    # JIT + AMP Combined
    print("\n" + "=" * 60)
    print("üîß OPTIMIZATION 4: JIT + AMP Combined")
    print("=" * 60)
    
    if device.type == 'cuda':
        try:
            from torch.cuda.amp import autocast
            
            model_fresh = models.resnet50(weights=None).to(device)
            model_fresh.eval()
            
            # First trace with FP16
            with torch.no_grad():
                with autocast(dtype=torch.float16):
                    traced_amp = torch.jit.trace(model_fresh, x.half())
                    traced_amp = torch.jit.optimize_for_inference(traced_amp)
                    traced_amp = torch.jit.freeze(traced_amp)
            
            # Benchmark
            times = []
            with torch.no_grad():
                x_half = x.half()
                
                # Warmup
                for _ in range(20):
                    _ = traced_amp(x_half)
                torch.cuda.synchronize()
                
                # Benchmark
                for _ in range(100):
                    start = torch.cuda.Event(enable_timing=True)
                    end = torch.cuda.Event(enable_timing=True)
                    start.record()
                    _ = traced_amp(x_half)
                    end.record()
                    torch.cuda.synchronize()
                    times.append(start.elapsed_time(end))
            
            jit_amp_ms = sorted(times)[len(times) // 2]
            results['jit_amp'] = jit_amp_ms
            
            print(f"\nüìà JIT + FP16:")
            print(f"   Median: {jit_amp_ms:.2f} ms")
            print(f"   ‚ö° Speedup: {baseline_ms / jit_amp_ms:.2f}x")
            
        except Exception as e:
            print(f"   ‚ùå JIT+AMP failed: {e}")
    
    # torch.compile (if PyTorch 2.x)
    print("\n" + "=" * 60)
    print("üîß OPTIMIZATION 5: torch.compile")
    print("=" * 60)
    
    if hasattr(torch, 'compile') and device.type == 'cuda':
        try:
            model_compile = models.resnet50(weights=None).to(device)
            model_compile.eval()
            
            compiled = torch.compile(model_compile, mode="reduce-overhead")
            
            # Warmup (compile happens lazily)
            with torch.no_grad():
                for _ in range(5):
                    _ = compiled(x)
            torch.cuda.synchronize()
            
            compile_ms = benchmark_model(compiled, x, "torch.compile")
            results['compile'] = compile_ms
            print(f"   ‚ö° Speedup: {baseline_ms / compile_ms:.2f}x")
            
        except Exception as e:
            print(f"   ‚ùå torch.compile failed: {e}")
    else:
        print("   ‚ö†Ô∏è torch.compile not available or no CUDA")
    
    # ========================================
    # Summary
    # ========================================
    print("\n" + "=" * 60)
    print("üìä OPTIMIZATION SUMMARY")
    print("=" * 60)
    
    print(f"\n{'Optimization':<25} {'Time (ms)':<12} {'Speedup':<10}")
    print("-" * 50)
    
    baseline = results.get('baseline', 1)
    for name, time_ms in results.items():
        speedup = baseline / time_ms if time_ms > 0 else 0
        print(f"{name:<25} {time_ms:>10.2f} {speedup:>8.2f}x")
    
    # Best result
    best_name = min(results, key=results.get)
    best_time = results[best_name]
    best_speedup = baseline / best_time
    
    print("\n" + "=" * 60)
    print(f"üèÜ BEST: {best_name} with {best_speedup:.2f}x speedup")
    print(f"   {baseline:.2f} ms ‚Üí {best_time:.2f} ms")
    print("=" * 60)
    
    # Clean up
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return results


if __name__ == "__main__":
    results = main()
